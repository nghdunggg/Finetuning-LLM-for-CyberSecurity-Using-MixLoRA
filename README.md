Fine-tuning Mistral-7B-Instruct-v0.3 with QLoRA
ƒê√¢y l√† d·ª± √°n fine-tuning m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn Mistral-7B-Instruct-v0.3 s·ª≠ d·ª•ng k·ªπ thu·∫≠t QLoRA (Quantized Low-Rank Adaptation). QLoRA cho ph√©p hu·∫•n luy·ªán m√¥ h√¨nh 7B m·ªôt c√°ch hi·ªáu qu·∫£ tr√™n c√°c GPU c√≥ b·ªô nh·ªõ VRAM h·∫°n ch·∫ø (v√≠ d·ª•: NVIDIA T4) b·∫±ng c√°ch s·ª≠ d·ª•ng l∆∞·ª£ng t·ª≠ h√≥a 4-bit.

M·ª•c ti√™u c·ªßa d·ª± √°n l√† hu·∫•n luy·ªán m√¥ h√¨nh c∆° s·ªü v·ªõi t·∫≠p d·ªØ li·ªáu t√πy ch·ªânh (normalized_merged_data.jsonl) cho c√°c t√°c v·ª• chuy√™n bi·ªát, sau ƒë√≥ h·ª£p nh·∫•t (merge) adapter LoRA v·ªõi m√¥ h√¨nh g·ªëc ƒë·ªÉ t·∫°o ra m·ªôt m√¥ h√¨nh ƒë·ªôc l·∫≠p, s·∫µn s√†ng cho vi·ªác chuy·ªÉn ƒë·ªïi sang GGUF ho·∫∑c suy lu·∫≠n.

üíª 1. C√†i ƒë·∫∑t v√† Kh·ªüi t·∫°o M√¥i tr∆∞·ªùng
C√†i ƒë·∫∑t Dependencies
S·ª≠ d·ª•ng file requirements.txt ƒëi k√®m ƒë·ªÉ c√†i ƒë·∫∑t t·∫•t c·∫£ c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt:

pip install -r requirements.txt

ƒêƒÉng nh·∫≠p Hugging Face
C·∫ßn ƒëƒÉng nh·∫≠p v√†o Hugging Face Hub ƒë·ªÉ t·∫£i m√¥ h√¨nh v√† b√°o c√°o log hu·∫•n luy·ªán (n·∫øu s·ª≠ d·ª•ng WandB):

from huggingface_hub import login
# Thay "HF_TOKEN" b·∫±ng token truy c·∫≠p c√° nh√¢n c·ªßa b·∫°n
login("HF_TOKEN")

# Thi·∫øt l·∫≠p th∆∞ m·ª•c cache t√πy ch·ªânh (n·∫øu c·∫ßn)
import os
os.environ["HF_HUB_CACHE"] = "/data/llm_cyber/hf_cache_2"

‚öôÔ∏è 2. C·∫•u h√¨nh Hu·∫•n luy·ªán (QLoRA & SFT)
Model v√† Quantization
M√¥ h√¨nh ƒë∆∞·ª£c t·∫£i v·ªõi c·∫•u h√¨nh l∆∞·ª£ng t·ª≠ h√≥a 4-bit (BitsAndBytesConfig) v√† s·ª≠ d·ª•ng bfloat16 cho t√≠nh to√°n:

C·∫•u h√¨nh

M√¥ t·∫£

Base Model

mistralai/Mistral-7B-Instruct-v0.3

Quantization

4-bit (QLoRA)

Compute Dtype

torch.bfloat16

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4", # Normal Float 4
    bnb_4bit_compute_dtype=torch.bfloat16,
)

LoRA Configuration (LoraConfig)
C·∫•u h√¨nh LoRA ƒë∆∞·ª£c √°p d·ª•ng cho t·∫•t c·∫£ c√°c module t·ª± ch√∫ √Ω v√† feed-forward c·ªßa m√¥ h√¨nh Mistral ƒë·ªÉ t·ªëi ƒëa h√≥a kh·∫£ nƒÉng h·ªçc:

Tham s·ªë

Gi√° tr·ªã

M·ª•c ƒë√≠ch

r

8

Rank c·ªßa ma tr·∫≠n LoRA.

lora_alpha

16

H·ªá s·ªë t·ª∑ l·ªá Œ±.

target_modules

["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

C√°c module ƒë∆∞·ª£c LoRA √°p d·ª•ng.

lora_dropout

0.05

Dropout rate.

D·ªØ li·ªáu v√† ƒê·ªãnh d·∫°ng
D·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i t·ª´ file JSON Lines c·ª•c b·ªô (/data/llm_cyber/normalized_merged_data.jsonl) v√† ƒë·ªãnh d·∫°ng theo ti√™u chu·∫©n chat c·ªßa Mistral:

# T·∫£i d·ªØ li·ªáu t·ª´ local
dataset = load_dataset("json", data_files="/data/llm_cyber/normalized_merged_data.jsonl", split="train")

# ƒê·ªãnh d·∫°ng d·ªØ li·ªáu cho Mistral:
# <s>[INST] {instruction} [/INST] {response}</s>
def format_data_for_mistral(example):
    text = f"<s>[INST] {example['instruction']} [/INST] {example['response']}</s>"
    return {"text": text}

Training Arguments (TrainingArguments)
Tham s·ªë

Gi√° tr·ªã

M√¥ t·∫£

output_dir

./mistral-mixlora-finetune

Th∆∞ m·ª•c l∆∞u checkpoint.

per_device_train_batch_size

2

Batch size tr√™n m·ªói thi·∫øt b·ªã.

gradient_accumulation_steps

8

TƒÉng batch size hi·ªáu qu·∫£ l√™n 16.

learning_rate

2e-2

T·ªëc ƒë·ªô h·ªçc.

max_steps

1000

S·ªë b∆∞·ªõc hu·∫•n luy·ªán t·ªëi ƒëa.

gradient_checkpointing

True

K·ªπ thu·∫≠t ti·∫øt ki·ªám VRAM.

report_to

"wandb"

B√°o c√°o log l√™n Weights & Biases.

üíæ 3. H·ª£p nh·∫•t LoRA Adapter (Merge)
Sau khi ho√†n t·∫•t fine-tuning, adapter LoRA ƒë∆∞·ª£c h·ª£p nh·∫•t v·ªõi m√¥ h√¨nh c∆° s·ªü ƒë·ªÉ t·∫°o th√†nh m·ªôt m√¥ h√¨nh FP16 ƒë·∫ßy ƒë·ªß, s·∫µn s√†ng cho suy lu·∫≠n.

L∆∞u √Ω quan tr·ªçng: Qu√° tr√¨nh h·ª£p nh·∫•t ƒë∆∞·ª£c th·ª±c hi·ªán ho√†n to√†n tr√™n CPU/RAM (device_map="cpu") do m√¥ h√¨nh 7B (FP16) y√™u c·∫ßu b·ªô nh·ªõ l·ªõn (kho·∫£ng 15GB).

C·∫•u h√¨nh Merge
Tham s·ªë

Gi√° tr·ªã

M√¥ t·∫£

BASE_MODEL_ID

"mistralai/Mistral-7B-Instruct-v0.3"

M√¥ h√¨nh g·ªëc.

LORA_ADAPTER_PATH

(C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n c·ªßa b·∫°n)

ƒê∆∞·ªùng d·∫´n t·ªõi checkpoint LoRA ƒë√£ hu·∫•n luy·ªán (v√≠ d·ª•: checkpoint-300).

OUTPUT_MERGED_MODEL_DIR

/data/llm_cyber/mistral_merged_model_gguf_ready

Th∆∞ m·ª•c l∆∞u m√¥ h√¨nh FP16 ƒë√£ h·ª£p nh·∫•t.

Code Snippet (Merge)
# 1. T·∫£i Base Model v√†o CPU RAM (torch_dtype=torch.float16, device_map="cpu")
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch # ƒê·∫£m b·∫£o ƒë√£ import torch

# X√≥a cache VRAM tr∆∞·ªõc khi t·∫£i m√¥ h√¨nh c∆° s·ªü l·ªõn
if torch.cuda.is_available():
    torch.cuda.empty_cache()

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    torch_dtype=torch.float16,
    device_map="cpu",          
    low_cpu_mem_usage=True,
    # ... c√°c tham s·ªë kh√°c
)
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)

# 2. T·∫£i LoRA Adapter
model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)

# 3. H·ª£p nh·∫•t (Merge)
model = model.merge_and_unload()

# 4. L∆∞u m√¥ h√¨nh ƒë√£ h·ª£p nh·∫•t (FP16)
model.save_pretrained(OUTPUT_MERGED_MODEL_DIR, safe_serialization=True)
tokenizer.save_pretrained(OUTPUT_MERGED_MODEL_DIR)
